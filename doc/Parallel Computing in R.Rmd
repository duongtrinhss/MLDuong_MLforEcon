---
title: "Parallel Computing in R"
author: "Duong Trinh"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: hide
  pdf_document: 
    number_sections: true
    extra_dependencies: ["mathtools","bbm"]
fontsize: 10pt
---


```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = function() {
  if (knitr::is_latex_output()) 'latex' else 'pandoc'
})
```

<!--- For HTML Only --->
`r if (knitr:::is_html_output()) '
$\\newcommand{\\mathbbm}[1]{\\mathbb{#1}}$
'`

# Introduction

Reference: 

+ Paper: [Parallel and Other Simulations in R Made Easy: An End-to-End Study](https://www.jstatsoft.org/article/view/v069i04)

+ Manual: [`simsalapar' package](https://cran.r-project.org/web/packages/simsalapar/simsalapar.pdf)

```{r}
library(parallel)
library(simsalapar)
```

# A simulation study

3 parts:

 1. *Setup*

 2. *Conducting the simulation*

 3. *Analyzing the results*

## Setup

```{r}
varList <- varlist(
  n.sim = list(type = "N", expr = quote(N[sim]), value = 10),
  n = list(type = "grid", value = c(75,100)),
  p = list(type = "grid", value = c(50,150))
)

toLatex(varList, label = "tab:var", 
        caption = "Variables which determine our simulation study.")

```


```{r}
str(getEl(varList))
```

```{r}
# Support Packages
library(tidyverse)
library(purrr)
library(ggplot2)
library(gridExtra)
```

```{r}
# Data Generating Process
# Note: library(pracma) # for a (non-symmetric) Toeplitz matrix
GenRegr <- function(n,p) {
  options.corr = 0
  options.R2 = 0.4
  options.epsilon = 0
  options.rho = NA
  
  # Generate predictors x
  if (options.corr == 0) {# Uncorrelated predictors
    x = matrix(rnorm(n*p),n,p)
  }
  else if (options.corr == 1) {# Spatially ncorrelated predictors
    C = toeplitz(options.rho^(0:(p-1)))
    x = matrix(rnorm(n*p),n,p)%*%chol(C)
  }
  else {
    print('Wrong choice of options.corr')
  }
  
  x <- data.matrix(sapply(data.frame(x), function(x) {(x-mean(x))/sd(x)})) # Standardize x
  
  # Generate coefficients
  beta <- rep(0,p)
  beta[1:6] <- c(1.5,-1.5,2,-2,2.5,-2.5)
  
  if (options.corr == 0) {
    signal_y <- sum(beta^2)
  } 
  else if (options.corr == 1) {
    signal_y <- sum((chol(C)%*%beta)^2)
  }
  
  c <- signal_y*((1-options.R2)/options.R2) # mean(sigmasq) is c to obtain desirable options.R2 (or SNR)
  
  # Generate epsilon
  if (options.epsilon == 0) { # iid error
    sigmasq <- c
  } 
  else if (options.epsilon == 1) {
    temp = (x%*%beta)
    sigmasq = c*temp/mean(temp)
  }
  
  epsilon = sqrt(sigmasq) * rnorm(n)
  
  # Generate y
  y = x%*%beta + epsilon
  
  return(list(y = y, x = x, beta = beta, sigmasq = sigmasq))
}
```


```{r}
library(Rcpp)
library(RcppArmadillo)
sourceCpp("/Users/duongtrinh/Dropbox/FIELDS/Data Science/R_Data Science/R Practice/CPPDuong/BayesRegrSP.cpp")

sourceCpp("/Users/duongtrinh/Dropbox/FIELDS/Data Science/R_Data Science/R Practice/CPPDuong/BayesRegr.cpp")
```


```{r}
doOne <- function(n, p, names = FALSE) 
{
  df <- GenRegr(n, p)
  y <- df$y
  X <- df$x
  beta_true <-  df$beta
  sigmasq_true <- df$sigmasq
  
  nsave <- 2000
  nburn <-  100
  
  beta_est <- array(rep(NA,p*nsave*2),dim = c(p,nsave,2))
  
  resRcpp_nosp <- BayesRegr(y,X,nsave=nsave,nburn=nburn)
  resRcpp_st <- BayesRegrSP(y,X,nsave=nsave,nburn=nburn,prior="student-t")
  beta_est[,,1] <- resRcpp_nosp$betadraws
  beta_est[,,2] <- resRcpp_st$betadraws
  
  return(beta_est)
}
```


```{r}
# res <- doLapply(varList, sfile = "res_lapply_seq.rds", doOne = doOne)

n <- 100
p <- 150
nsave <- 2000
Nsim <- 10
beta_true <- rep(0,p)
beta_true[1:6] <- c(1.5,-1.5,2,-2,2.5,-2.5)


library(parallel)
library(tictoc)
tic()
res1 <- lapply(1:Nsim, function(sim) doOne(n,p))
toc()

tic()
res2 <- mclapply(1:Nsim, function(sim) doOne(n,p),
                 mc.set.seed = TRUE,
                 mc.cores = getOption("mc.cores", 8L))
toc()


res3 <- array(unlist(res2), 
              dim = c(p,nsave,2,Nsim),
              dimnames = list(beta = map_chr(1:p,~paste0("b",.x)),
                              iter = map_chr(1:nsave,~paste0("iter",.x)),
                              method = c("NoShrinkage", "CTS-Studt"),
                              sim = map_chr(1:Nsim,~paste0("sim",.x))))
  
library(reshape2)
res4 <- melt(res3, value.name = "value")


```




## About variable selection

### Approach 1 - 2-means (2-M) variable selection

Doing variable selection using shrinkage priors by post-processing posterior samples of the regression coefficients.


**Algorithm**

Define $S_T$ to be the indices of non-zero signals in $\beta_T$ and $S_E$ to be the indices of the selected covariates.

Consider two types of errors:

  + $\mid S_T \cap S_E^C\mid$: masking error (false negative)
  + $\mid S_E \cap S_T^C \mid$: swamping error (false positive)
  
Let $b > 0$ be a tuning parameter, then *Sequential 2-means (S_2M)* is defined as follows: On the $i^{th}$ MCMC sample of $\beta$:

1. Perform a 2-means algorithm on $\mid \beta_j \mid, j = 1,2,\ldots,p$. Denote the cluster means with smaller values by $m$ and the one with larger values by $M$ ($m < M$). Initialize a set $D$ with all the indices from the cluster with the smaller mean $m$. While the difference $M - m$ is greater than $b$:

  (a) update $D$ to be all the indices from the cluster with the smaller mean $m$;
  (b) perform a 2-means on $\mid\beta_j\mid, j\in D$;
  (c) update $m$ and $M$ to be two cluster means ($m \leq M$) obtained from (b).
  
2. The set $D$ is considered to contain coefficients of noise covariates. So the estimated number of signals $h_i$ is $p - \mid D\mid$





```{r}
res5 <- res4 %>% filter(sim == "sim2", method == "NoShrinkage") %>% 
  select(value)
str(res5)

beta_post <- t(matrix(unlist(res5), 150, 2000))

resS2M <- S2M(beta_post,0,1,100)

Hbi.vs.bi(resS2M)

S2M.vs(resS2M,5)

```


```{r}
###################
#S2M main function#
###################


#This function creates a sequence $\{b_i\}$ of $l$ values of the tuning parameter and implements Sequential-2-Means on the posterior samples to obtain variable selection results corresponding to each value in the sequence $\{b_i\}$. }
#\arguments{
# \item{Beta}{
#   it is an $N$ by $p$ matrix consisting of $N$ posterior samples of $p$ variables
# }
# \item{lower}{
#   the lower bound of the chosen values of the tuning parameter
# }
# \item{upper}{
#   the upper bound of the chosen values of the tuning parameter
# }
# \item{l}{
#   the number of chosen values of the tuning parameter
# }


# \value{
#   \item{N }{the posterior sample size}
#   \item{p }{the total number of variables}
#   
#   \item{H.b.i }{the estimated number of signals corresponding to each $b_i$}
#   \item{b.i }{the values of the tuning parameter}
#   
#   \item{abs.post.median }{medians of the absolute values of the posterior samples of each variable}
#   
S2M=function(Beta,lower,upper,l)
{
  
  N=dim(Beta)[1]
  p=dim(Beta)[2]
  b.i=seq(lower,upper,length=l)
  H.b.i=NULL
  for(r in 1:l)
  {
    KK=NULL
    for (i in 1:N)
    {
      fit=kmeans(abs(Beta[i,]),2)
      cen1=min(fit$centers)
      cen2=max(fit$centers)
      temp1=Beta[i,]

      
      fit1=fit
      
      while(cen2-cen1>b.i[r])
      {
        fit1=fit
        temp=which.min(fit$centers)
        temp1=temp1[which(fit$cluster==temp)]
        if(length(temp1)<=2){break;}
        fit=kmeans(abs(temp1),2)
        cen2=max(fit$centers)
        cen1=min(fit$centers)
      }
      temp=which.min(fit1$centers)
      KK[i]=p-length(which(fit1$cluster==temp))
    }
    H.b.i[r]=as.numeric(names(sort(-table(KK)))[1])
  }
  abs.post.median=NULL
  for(i in 1:p)
  {
    abs.post.median[i]=median(abs(Beta[,i]))
  }
  return(list(N=N,
              p=p,
              H.b.i=H.b.i,
              b.i=b.i,
              abs.post.median=abs.post.median
    ))
}



############################
#make a H_b_i v.s. b_i plot#
############################
#This function is to make the $H_{b_{i}}$-v.s.-$b_i$ plot. The estimated number of signals, or equivalently, the optimal value of the tuning parameter can be decided by looking at this plot.
# \arguments{
#   \item{S2M}{a list obtained from the function \code{S2M}.}
# }
Hbi.vs.bi=function(S2M)
{
  plot(S2M$b.i,S2M$H.b.i)
}




###############################
#variable selection results   #
#given a particular value of H#
###############################
#This function is to print out the variable selection results given the estimated number of signals.}
# \arguments{
#   \item{S2M}{a list obtained from the function \code{S2M}.}
#   \item{H}{the estimated number of signals}
# }
# \value{
#   \item{var.selected}{The indices of selected variables}
#  }

S2M.vs=function(S2M,H)
{
  p=S2M$p
  abs.post.median=S2M$abs.post.median
  var.selected=order(abs.post.median)[p:(p-H+1)]
  return(var.selected)
}
```


### Approach 2: Using reference models in variable selection

### Approach 3: Selection by posterior credible intervals

Selecting those variables whose posterior distribution does not include zero in the interval between the a% and (1-a)% quantiles

Compelete selection

Minimal 








